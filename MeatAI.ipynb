{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MeatAI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-Sunmoon2021/ImageProcessingProjects/blob/main/MeatAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HmHreC4Hs0a"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## **목적** >  **이미지 분류 모델을 통한 고기 판별 Web앱 만들기**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "학습시킬 이미지 양이 너무 부족해 사전의 학습된 VGG16 신경망을 활용하여 전이학습을 진행함.\n",
        "\n",
        "---\n",
        "*  Keras습특\n",
        "*  CNN 이해\n",
        "*  Keras 이전학습된 모델 VGG16를 이용항 클래스 분류\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **개요**\n",
        "*  데이터셋：ImageNet\n",
        "*  네트워크：VGG16\n",
        "*  실행환경：windows10, Google Colaboratory（GPU）\n",
        "*  최적화 알고리즘 :RMSpropGraves\n",
        "\n",
        "---\n",
        "\n",
        "## **ImageNet**\n",
        "*  1400만이미지、2만 클래스의 대이터셋\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **VGG16**\n",
        "*  ImageNet에서 추출된 이미지 (1000클래스)로 학습한 모델\n",
        "*  전결합층×3、합성곱층×13의 16층 뉴럴 네트워크\n",
        "\n",
        "* 큰 단점:\n",
        "\n",
        "1.   학습이 매우 느리다>이유는 파라미터가 많기 때문에.\n",
        "2.   용량이 엄청나게 크다>많은 파라미터 때문에 생기는 단점.\n",
        "\n",
        "VGG는 각각 533MB, 574MB 이다. 케라스로 직접 받으면 상당히 오래 걸리고 하드용량이 1기가가 줄어듬.\n",
        "\n",
        "다행히 다음 모델들부터는 이 단점이 개선되었음.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KQP7duzJprp"
      },
      "source": [
        "## **최적화 (Optimization):**\n",
        "\n",
        "1.   함수 등을 최적의 상태로 접근하는 것. \n",
        "2.   깊은 학습의 최적화는 모델의 예측 값과 실제 값과의 오차에서 매개 변수 (무게)를 업데이트하는 것.\n",
        "3.   그 최적화 기법은 다음과 같이 여러 가지가 있음.\n",
        "4.   매개 변수를 업데이트 할 때 수식이 각각 다름.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVjBpcRAqPQZ"
      },
      "source": [
        "# **SGD:**\n",
        "SGD 와 Stochastic Gradient Descent의 약자로 일본어로는 확률 적 기울기 강하 법 이라고합니다. 가장 기본적인 알고리즘 입니다. 확률 적 기울기 강하 법 은 훈련 데이터로부터 하나씩 데이터를 무작위로 추출하여 그 데이터 점의 기울기를 계산하고 매개 변수를 업데이트합니다. 자세한 내용은 여기 를 참조하십시오.\n",
        "\n",
        "> インデントされたブロック\n",
        "\n",
        "\n",
        "\n",
        "# **MomentumSGD:**\n",
        "MomentumSGD은 SGD 관성 항을 부여한 것입니다. 관성 항 (Momentum : 모멘텀)은 경사 강하 법의 식을 확장하여보다 신속하게 수렴하도록 고안하기위한 매개 변수로 존재하고 있습니다.\n",
        "\n",
        "> インデントされたブロック\n",
        "\n",
        "\n",
        "\n",
        "# **AdaGrad:**\n",
        "논문:http://jmlr.org/papers/v12/duchi11a.html\n",
        "\n",
        "> インデントされたブロック\n",
        "\n",
        "\n",
        "\n",
        "# **AdaDelta:**\n",
        "논문:http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf\n",
        "\n",
        "> インデントされたブロック\n",
        "\n",
        "\n",
        "\n",
        "# **Adam:**\n",
        "논문:[1412.6980v8] Adam: A Method for Stochastic Optimization\n",
        "     https://arxiv.org/abs/1412.6980v8\n",
        "     \n",
        "> インデントされたブロック\n",
        "\n",
        "\n",
        "\n",
        "# **RMSpropGraves:**\n",
        "논문:[1308.0850] Generating Sequences With Recurrent Neural Networks\n",
        "     https://arxiv.org/abs/1308.0850\n",
        "\n",
        "> インデントされたブロック\n",
        "\n",
        "\n",
        "\n",
        "# **NesterovAG:**\n",
        "논문:[1212.0901] Advances in Optimizing Recurrent Networks\n",
        "     https://arxiv.org/abs/1212.0901\n",
        "\n",
        "\n",
        "> インデントされたブロック\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlWTCThQ8vQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74c7db6-ad28-4d25-e2f7-451168eaf120"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBpYy3lHkmKR"
      },
      "source": [
        "import os\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
        "from keras import optimizers\n",
        "from keras.applications.vgg16 import VGG16\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3ig0N-Jxkoi3",
        "outputId": "f8b360fa-ff8e-4047-eabe-0451887b9b54"
      },
      "source": [
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8qxpp4NqrNh"
      },
      "source": [
        "**訓練画像、検証画像、テスト画像のディレクトリ/훈련이미지, 검층이미지, 테스트이미지 디렉토리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He2eIGMckwLR"
      },
      "source": [
        "# 分類クラス\n",
        "classes = ['rare', 'welldone']\n",
        "nb_classes = len(classes)\n",
        "batch_size_for_data_generator = 20\n",
        "\n",
        "base_dir = \".\"\n",
        "\n",
        "train_dir = os.path.join(base_dir, '/content/drive/MyDrive/Colab Notebooks/data/train')\n",
        "validation_dir = os.path.join(base_dir, '/content/drive/MyDrive/Colab Notebooks/data/validation')\n",
        "test_dir = os.path.join(base_dir, '/content/drive/MyDrive/Colab Notebooks/data/test')\n",
        "\n",
        "train_rare_dir = os.path.join(train_dir, 'rare')\n",
        "train_welldone_dir = os.path.join(train_dir, 'welldone')\n",
        "\n",
        "validation_rare_dir = os.path.join(validation_dir, 'rare')\n",
        "validation_welldone_dir = os.path.join(validation_dir, 'welldone')\n",
        "\n",
        "test_rare_dir = os.path.join(test_dir, 'rare')\n",
        "test_welldone_dir = os.path.join(test_dir, 'welldone')\n",
        "\n",
        "# 画像サイズ\n",
        "img_rows, img_cols = 200, 200\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOfCDuPcqOWo"
      },
      "source": [
        "**画像データの数を確認する/이미지데이터가 얼마나 있는지 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "248rpEsdnfMo",
        "outputId": "8c08eda3-bd25-49e0-88b1-530d4f4ad786"
      },
      "source": [
        "print('total training rare images:', len(os.listdir(train_rare_dir )),train_rare_dir )\n",
        "print('total training welldone images:', len(os.listdir(train_welldone_dir)),train_welldone_dir)\n",
        "\n",
        "print('total validation rare images:', len(os.listdir(validation_rare_dir)),validation_rare_dir)\n",
        "print('total validation welldone images:', len(os.listdir(validation_welldone_dir)),validation_welldone_dir)\n",
        "\n",
        "print('total test rare images:', len(os.listdir(test_rare_dir)),test_rare_dir)\n",
        "print('total test welldone images:', len(os.listdir(test_welldone_dir)),test_welldone_dir)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training rare images: 330 /content/drive/MyDrive/Colab Notebooks/data/train/rare\n",
            "total training welldone images: 220 /content/drive/MyDrive/Colab Notebooks/data/train/welldone\n",
            "total validation rare images: 20 /content/drive/MyDrive/Colab Notebooks/data/validation/rare\n",
            "total validation welldone images: 19 /content/drive/MyDrive/Colab Notebooks/data/validation/welldone\n",
            "total test rare images: 1 /content/drive/MyDrive/Colab Notebooks/data/test/rare\n",
            "total test welldone images: 1 /content/drive/MyDrive/Colab Notebooks/data/test/welldone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBfoq2m4olVY"
      },
      "source": [
        "**ImageDataGeneratorを使って画像データを拡張する/ImageDataGenerator를 사용하여 이미지데이터를 확장시킴**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYj6BM0iofIB",
        "outputId": "438d1b16-a9f1-41cc-ee7f-5520a73298ff"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1.0 / 255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(directory=train_dir,target_size=(img_rows, img_cols),color_mode='rgb',classes=classes,class_mode='categorical',batch_size=batch_size_for_data_generator,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 550 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbIRlBDso2zf",
        "outputId": "32891003-916f-439a-aec0-8ba4ac4875a5"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "    \n",
        "validation_generator = test_datagen.flow_from_directory(directory=validation_dir,target_size=(img_rows, img_cols),color_mode='rgb',classes=classes,class_mode='categorical',batch_size=batch_size_for_data_generator,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 39 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4snczifpEh2"
      },
      "source": [
        "**VGG16モデル/vgg16모델**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bXuaWyco8C3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "206e0cc4-eb32-444c-a3eb-45d17238921d"
      },
      "source": [
        "input_tensor = Input(shape=(img_rows, img_cols, 3))\n",
        "vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
        "vgg16.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 200, 200, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 200, 200, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 200, 200, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 100, 100, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 100, 100, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 100, 100, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 50, 50, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 50, 50, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 50, 50, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 50, 50, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 25, 25, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 25, 25, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 25, 25, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 25, 25, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMM25CpepGT3"
      },
      "source": [
        "**VGG16モデルに全結合分類器を追加する/vgg16모델에 전결합분류기를 추가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ocguBcApI12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381cf991-5134-4532-a1ff-5729e2d1142e"
      },
      "source": [
        "top_model = Sequential()\n",
        "#model.add(vgg16)\n",
        "#model.add(Flatten())\n",
        "top_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\n",
        "top_model.add(Dense(256, activation='relu'))\n",
        "top_model.add(Dropout(0.5))\n",
        "top_model.add(Dense(nb_classes, activation='softmax'))\n",
        "model = Model(inputs=vgg16.input, outputs=top_model(vgg16.output))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 200, 200, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 200, 200, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 200, 200, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 100, 100, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 100, 100, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 100, 100, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 50, 50, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 50, 50, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 50, 50, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 50, 50, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 25, 25, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 25, 25, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 25, 25, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 25, 25, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 2)                 4719362   \n",
            "=================================================================\n",
            "Total params: 19,434,050\n",
            "Trainable params: 19,434,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cPOdzqtpPoP"
      },
      "source": [
        "**VGG16のblock5_conv1以降と追加した全結合分類器のみ訓練する/vgg16의 block5_conv1이후와 추가한 전결합분류기만을 훈련시킴**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2hpXJ1BpNhZ"
      },
      "source": [
        "vgg16.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in vgg16.layers:\n",
        "    if layer.name == 'block5_conv1':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdI4YL9vpVqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0a93b9-da96-445f-926f-523ed494d387"
      },
      "source": [
        "for layer in vgg16.layers:\n",
        "    print(layer, layer.trainable )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7fa8671fba90> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8679de950> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8673013d0> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fa8631c7f50> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631d0d10> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631d0190> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fa8631da990> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631ddfd0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631e58d0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631d6f50> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fa8631efcd0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631f8fd0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631f8410> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa86317fe90> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fa863185f90> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa863189110> True\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa863180cd0> True\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa867ac9150> True\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fa867b6e610> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx9suZrapYLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f21bc8-d351-4a26-c1ab-ce9d0736ee1b"
      },
      "source": [
        "for layer in model.layers:\n",
        "    print(layer, layer.trainable )\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7fa8671fba90> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8679de950> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8673013d0> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fa8631c7f50> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631d0d10> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631d0190> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fa8631da990> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631ddfd0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631e58d0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631d6f50> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fa8631efcd0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631f8fd0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa8631f8410> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa86317fe90> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fa863185f90> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa863189110> True\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa863180cd0> True\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fa867ac9150> True\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7fa867b6e610> True\n",
            "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa8671f6250> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx0K5TuGpbSZ"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer=optimizers.RMSprop(lr=1e-5), metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA2JJ9xpfe3"
      },
      "source": [
        "**学習/학습**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_syZJI6JpeJw",
        "outputId": "28660eaa-14a8-486c-c5df-29e40beec0bc"
      },
      "source": [
        "history = model.fit_generator(train_generator,steps_per_epoch=25,epochs=30,validation_data=validation_generator,validation_steps=10,verbose=1)   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "25/25 [==============================] - ETA: 0s - loss: 0.6706 - acc: 0.6095WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
            "25/25 [==============================] - 251s 10s/step - loss: 0.6668 - acc: 0.6124 - val_loss: 0.5680 - val_acc: 0.8205\n",
            "Epoch 2/30\n",
            "25/25 [==============================] - 220s 9s/step - loss: 0.3590 - acc: 0.8498\n",
            "Epoch 3/30\n",
            "25/25 [==============================] - 218s 9s/step - loss: 0.2314 - acc: 0.9088\n",
            "Epoch 4/30\n",
            "25/25 [==============================] - 221s 9s/step - loss: 0.1382 - acc: 0.9523\n",
            "Epoch 5/30\n",
            "25/25 [==============================] - 219s 9s/step - loss: 0.1151 - acc: 0.9627\n",
            "Epoch 6/30\n",
            "25/25 [==============================] - 219s 9s/step - loss: 0.0678 - acc: 0.9757\n",
            "Epoch 7/30\n",
            "25/25 [==============================] - 220s 9s/step - loss: 0.0472 - acc: 0.9820\n",
            "Epoch 8/30\n",
            "25/25 [==============================] - 224s 9s/step - loss: 0.0444 - acc: 0.9842\n",
            "Epoch 9/30\n",
            "25/25 [==============================] - 221s 9s/step - loss: 0.0220 - acc: 0.9959\n",
            "Epoch 10/30\n",
            "25/25 [==============================] - 221s 9s/step - loss: 0.0192 - acc: 0.9963\n",
            "Epoch 11/30\n",
            "25/25 [==============================] - 222s 9s/step - loss: 0.0171 - acc: 0.9882\n",
            "Epoch 12/30\n",
            "25/25 [==============================] - 223s 9s/step - loss: 0.0157 - acc: 0.9953\n",
            "Epoch 13/30\n",
            "25/25 [==============================] - 225s 9s/step - loss: 0.0146 - acc: 0.9961\n",
            "Epoch 14/30\n",
            "25/25 [==============================] - 222s 9s/step - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 15/30\n",
            "25/25 [==============================] - 222s 9s/step - loss: 0.0073 - acc: 0.9976\n",
            "Epoch 16/30\n",
            "25/25 [==============================] - 222s 9s/step - loss: 0.0216 - acc: 0.9877\n",
            "Epoch 17/30\n",
            "25/25 [==============================] - 223s 9s/step - loss: 0.0066 - acc: 0.9961\n",
            "Epoch 18/30\n",
            "25/25 [==============================] - 222s 9s/step - loss: 0.0030 - acc: 1.0000\n",
            "Epoch 19/30\n",
            "25/25 [==============================] - 225s 9s/step - loss: 9.9927e-04 - acc: 0.9998\n",
            "Epoch 20/30\n",
            "25/25 [==============================] - 222s 9s/step - loss: 0.0113 - acc: 0.9926\n",
            "Epoch 21/30\n",
            "25/25 [==============================] - 226s 9s/step - loss: 3.1891e-04 - acc: 1.0000\n",
            "Epoch 22/30\n",
            "25/25 [==============================] - 227s 9s/step - loss: 0.0030 - acc: 1.0000\n",
            "Epoch 23/30\n",
            "25/25 [==============================] - 223s 9s/step - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 24/30\n",
            "25/25 [==============================] - 222s 9s/step - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 25/30\n",
            "25/25 [==============================] - 223s 9s/step - loss: 4.6266e-04 - acc: 1.0000\n",
            "Epoch 26/30\n",
            "25/25 [==============================] - 223s 9s/step - loss: 3.4872e-04 - acc: 1.0000\n",
            "Epoch 27/30\n",
            "25/25 [==============================] - 223s 9s/step - loss: 0.0137 - acc: 0.9961\n",
            "Epoch 28/30\n",
            "25/25 [==============================] - 222s 9s/step - loss: 3.0356e-04 - acc: 1.0000\n",
            "Epoch 29/30\n",
            "25/25 [==============================] - 223s 9s/step - loss: 3.7549e-04 - acc: 1.0000\n",
            "Epoch 30/30\n",
            "25/25 [==============================] - 224s 9s/step - loss: 1.1400e-04 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vR8-IejpnGP"
      },
      "source": [
        "**学習結果を保存する/학습결과를 저장함**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGnEfF_opmmI"
      },
      "source": [
        "hdf5_file = os.path.join(base_dir, 'meats-model.hdf5')\n",
        "model.save_weights(hdf5_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx2RGi1ksOzH"
      },
      "source": [
        "**学習推移をグラフに表示する/학습추이를 그래프로 나타냄**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-f3fgfzsWDP"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbhPVFYNsbft"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV_HgKLQscC1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "660067c7-4b80-4f34-c267-3ed95fd1a959"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-4ce54a883635>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training and validation accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (30,) and (1,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIBPeco6cSTa"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trLto9tgshDe"
      },
      "source": [
        "**テストの画像データで正解率を調べる/테스트의 이미지데이터를 가져오고 정확률을 확인함**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0m2suifsgie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0965f5-a214-4e75-f030-6847e55a4e92"
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(directory=test_dir,target_size=(img_rows, img_cols),color_mode='rgb',classes=classes,class_mode='categorical',batch_size=batch_size_for_data_generator)\n",
        "\n",
        "test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n",
        "print('test acc:', test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n",
            "test acc: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZM_eV2FsvoO"
      },
      "source": [
        "**実際にテスト画像を分離してみる/실체로 테스트이미지를 분리함**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpASTKtNsgZe"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tNC0o7cs1gO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67136d30-d015-4aad-f618-baf331eef446"
      },
      "source": [
        "filename = os.path.join(test_dir, 'rare')\n",
        "filename = os.path.join(filename, 'rare_test1.jpg')\n",
        "filename"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/data/test/rare/rare_test1.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3rm2yFDtB5t"
      },
      "source": [
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Eq_nCWtCpO"
      },
      "source": [
        "img = np.array( Image.open(filename))\n",
        "plt.imshow( img )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HgueoCctHFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15ad065-c0ea-4475-f347-59e030f8a671"
      },
      "source": [
        "img = load_img(filename, target_size=(img_rows, img_cols))\n",
        "x = img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "predict = model.predict(preprocess_input(x))\n",
        "for pre in predict:\n",
        "    y = pre.argmax()\n",
        "    print(\"test result=\",classes[y], pre)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test result= welldone [1.2407979e-24 1.0000000e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}